# The Archival Interface: A Technical Framework for 1990–2012 Web Retrieval

## 1. Epistemological Context: The Digital Transition (1990–2012)

The interval between 1990 and 2012 represents the most volatile period in the history of recorded information. It marks the complete migration of human knowledge production from analog substrates—newsprint, physical memos, broadcast tapes—to networked, digital architectures. For the digital archivist, this era is not merely a chronological designation but a distinct technological problem set characterized by the collision of two opposing forces: the exponential explosion of content creation and the rudimentary, often ephemeral, nature of early storage and indexing protocols.

This report provides a comprehensive technical analysis and implementation framework for retrieving content from this specific window. The objective is to construct a unified retrieval matrix using free, public Application Programming Interfaces (APIs). The analysis targets six specific providers: The Internet Archive, The New York Times, The Guardian, GDELT, Google/Bing, and Chronicling America. The central challenge addressed herein is the retrieval of "at-risk" historical data—content that exists in the "Digital Dark Age" gap where "born-digital" content was created before modern archival standards (like WARC files and robust CMS backups) were universally adopted.

## 1.1 The Nature of the 1990–2012 Gap

To understand the technical requirements for API implementation, one must first characterize the data environment of the target era. The year 1990 predates the public World Wide Web. Content from 1990–1995 is almost exclusively digitized print media or early FTP/Gopher deposits. The Internet Archive did not begin its crawl operations until 1996, meaning the first quarter of the target window is invisible to web crawlers and exists only in the databases of legacy publishers like The New York Times.

From 1996 to 2005, the "Static Web" prevailed. Content was often hand-coded in HTML, linked via static file paths. This era is highly susceptible to "Link Rot." A URL structure from 1998 (e.g., <www.site.com/news/1998/nov/article1.html>) was likely destroyed during server migrations to dynamic Content Management Systems (CMS) in the mid-2000s.

From 2006 to 2012, the "Social Web" (Web 2.0) emerged. While volume increased, so did volatility. Dynamic URLs generated by databases (e.g., view_post.php?id=3492) became common, making archival retrieval difficult because metadata was often decoupled from the URL string itself. By 2012, the end of our target window, the web had centralized into walled

gardens, but the APIs discussed in this report primarily target the open web and journalistic record that preceded this enclosure.

## 1.2 The "Recency Bias" of Commercial Search

A recurring theme in this analysis is the inadequacy of commercial search engines for historical retrieval. As the detailed analysis in Section 6 will demonstrate, APIs provided by Google and Bing are optimized for "freshness".² They prioritize the current version of a page. If a URL from 2001 was updated in 2023, the search API will index the 2023 version, effectively erasing the historical record sought by the archivist. This necessitates a shift in strategy: moving away from "Discovery Engines" (Google/Bing) and toward "Preservation Engines" (Internet Archive, NYT).

The following sections rigorously evaluate each provider against the user's constraints: verifying historical reach, analyzing API syntax, identifying link rot risks, and determining the feasibility of free-tier access.

## 2. The Internet Archive: The Keystone of Digital Preservation

The Internet Archive (IA) is the only provider capable of resolving the "Link Rot" inherent in the 1990–2012 timeframe. Unlike publisher archives (NYT, Guardian) which hold their own content, or search engines (Google) which index current content, the IA holds snapshots of the entire accessible web. For the developer, the primary interface is the Wayback Machine CDX Server API.

## 2.1 The CDX Server API Architecture

The CDX API is a specialized HTTP servlet designed to query the index of captures (Wayback Machine snapshots) rather than the content itself.³ It returns metadata about what the archive holds, which is the necessary first step before retrieving the actual HTML.

The index format, known as CDX, contains fields that describe each record. The API is essential for the 1990–2012 window because it allows the archivist to verify if a URL existed at a specific point in time, even if the domain is currently defunct. The server responds to GET queries with plain text or JSON data, making it highly ingestible for Python-based harvesting pipelines.³

## 2.1.1 Essential Query Parameters

To effectively utilize the CDX API for the target era, the developer must master specific parameters that control the density and precision of the results.

- url (Required): The specific URL to search. The API supports prefix and domain-level searching using wildcards. A query for url=archive.org/* is equivalent to a prefix match, returning all URLs under that path.⁴
- from and to (Date Filtering): These parameters are critical for the 1990–2012 constraint. They accept timestamps in the format yyyyMMddhhmms. For broad filtering, a developer can supply just the year (e.g., from=1990&amp;to=2012). This ensures the query excludes the massive volume of data post-2012, reducing latency and processing load.³
- collapse (Density Control): A major challenge with the IA is duplicate data. A popular page might be crawled daily. For an archivist looking for unique content, receiving 365 snapshots for a single year is redundant. The collapse=digest parameter collapses adjacent lines that have the same SHA-1 content digest. This filters out duplicates where the page content hasn't changed, even if the timestamp has.⁴ Alternatively, collapse=timestamp:10 can be used to limit results to one capture per hour, or timestamp:6 for one per month.
- fl (Field Limiting): To reduce bandwidth, the fl parameter allows the developer to request only specific columns. For this project, the essential fields are timestamp, original (the source URL), mimetype (to filter for text/html), and statuscode (to ensure successful captures, usually 200).⁴

## 2.1.2 Handling "Soft 404s" and Redirection

The 1990–2012 web is full of redirects (301, 302). The CDX API returns a statuscode field. A robust implementation must filter for statuscode:200. Snippet³ indicates that the API explicitly logs these codes. A capture with status 302 means the archive merely recorded a redirect; the content is likely at a different URL. The Python implementation must account for this by either following the redirect chain or filtering strictly for 200 OK responses.

## 2.2 The Memento API

While the CDX API provides the index, the Memento API (RFC 7089) provides the standard for retrieving the content. Snippet highlights the utility of the wayback Python library, which interfaces with both APIs. The Memento API allows a developer to request a URL and a "Accept-Datetime" header. The server then redirects to the closest available snapshot.

However, for bulk harvesting of the 1990–2012 window, the CDX API is superior because it allows iteration over a list of captures. The Memento API is best used as a "fallback" mechanism—when a specific URL is known, and the archivist needs the version from June 1999.

## 2.3 Advanced Tooling: cdx_toolkit

For large-scale extraction, the standard REST requests can be cumbersome to manage (pagination, retries). Snippet⁵ introduces cdx_toolkit, a Python command-line tool and library designed for high-volume interaction with the CDX API. It supports iterating over captures

(cdxt iter) and handles the complexities of the matchType parameter. It is particularly useful for extracting all URLs from a specific domain (e.g., whitehouse.gov) between 1990 and 2012.

The toolkit abstracts the differences between the Internet Archive's CDX and Common Crawl's CDX (discussed later), providing a unified interface. It defaults to a limit of 1000 records to prevent over-fetching, a safety rail that is beneficial for free-tier users.⁵

## 2.4 Link Rot Mitigation Strategy

The primary value of the Internet Archive in this matrix is Link Rot Mitigation. When utilizing other APIs (like GDELT or NYT) that provide URLs from 1990–2012, a significant percentage of those links will fail (404) on the live web. The "Wayback Fallback" strategy involves taking the original URL and publication date provided by NYT/GDELT, and querying the CDX API for a snapshot closest to that date. This effectively resurrects the dead link, allowing the text analysis to proceed.

## 3. The New York Times: The Continuous Record (1851–Present)

Among the provider list, The New York Times (NYT) Article Search API stands out as the most robust source for text-based content dating back to the start of the target window (1990). Unlike web-native archives that rely on crawling, the NYT archive is a digitized database of the publication's own output, ensuring high metadata quality and zero "content drift."

## 3.1 Historical Reach and Data Fidelity

The NYT API provides access to article metadata from 1851 to the present.⁶ This fully encompasses the 1990–2012 requirement. For the 1990–1995 period (pre-Web), this is likely the only source in the requested list that will return high-quality results.

The data returned includes:

- Headlines: The original print headline.
- Abstracts: A summary of the article.
- Lead Paragraphs: Often sufficient for NLP analysis without needing the full text.
- Publication Dates: Precise to the minute.
- Keywords/Tags: Structured metadata (Person, Organization, GLocation).⁷

## 3.2 API Syntax and Filtering

The API uses a RESTful architecture with rich filtering capabilities via the fq (Filter Query) parameter.

- Date Filtering: This is the most critical constraint. The API uses begin_date and end_date parameters in the format YYYYMMDD. This aligns perfectly with the user's need to slice data from 19900101 to 20121231.
- Field Filtering (fl): To optimize response size, the user can request specific fields: fl=web_url, headline, pub_date, abstract. This is crucial for creating the lightweight matrix requested.
- Faceted Search: The fq parameter supports Lucene-like syntax. A user can filter by section_name (e.g., "Technology", "World") or type_of_material (e.g., "News", "Op-Ed"). This allows for precise segmentation of the archive (e.g., retrieving only "Front Page" news from 2001).

## 3.3 The Pagination Constraint

The most significant technical limitation of the NYT API is the Pagination limit. The API returns 10 results per page and allows pagination up to page 100. This means a single query can retrieve a maximum of 1,000 items (10 * 100).

Implication for 1990–2012: A query for "politics" between 1990 and 2012 will yield hundreds of thousands of results. The API will cut off access after the 1,000th item.

Solution: The Python implementation must use a Recursive Date Slicing strategy. If a query for 1990-01-01 to 2012-12-31 returns &gt;1,000 hits, the script must split the timeframe in half (e.g., 1990-2001 and 2001-2012) and re-query. This process repeats until the hit count for the time window is under 1,000, ensuring complete coverage.9

## 3.4 Rate Limiting and Free Tier

The NYT Developer Network is generous but strict:

- Limit: 10 requests per minute (4,000 per day).
- Mechanism: The API key acts as the throttle.
- Handling: The Python code must implement a time.sleep(6) delay between requests to remain compliant with the 10/minute limit. Violating this will result in a 429 Too Many Requests error.

## 4. The Guardian Open Platform: Born-Digital Archives (1999–2012)

The Guardian's Open Platform is a powerful counterweight to the NYT, offering a European perspective and a more "open" data philosophy. However, its utility is strictly bound by its digitization history.

## 4.1 The 1999 Start Date

The documentation $^{10}$ explicitly states that the API stores content "dating back to 1999." This

presents a gap for the 1990–1998 portion of the user's request. The Python implementation must account for this by either bypassing The Guardian for dates prior to 1999 or logging a warning. Requests for 1990–1998 will return zero results, which could be misinterpreted as a failure if not properly handled in the logic.

## 4.2 API Syntax and Rich Content

The Guardian API is notable for providing the full body content of articles, not just abstracts.

- Date Filtering: Uses from-date and to-date in ISO 8601 format (YYYY-MM-DD).¹¹ Note the difference from NYT (YYYYMMDD).
- Content Retrieval: The show-fields=all or show-fields=body parameter forces the API to return the HTML content of the article.¹¹ This is a massive advantage for archivists, as it removes the need to scrape the destination URL, thereby bypassing the Link Rot issue entirely for this provider.
- Tags and Sections: The API supports extensive tagging (tag=politics/politics, section=technology). This allows for high-precision querying similar to the NYT's faceted search.¹²

## 4.3 Rate Limits and Tiers

The Guardian offers a "Developer Key" (Free):

- Limit: 12 calls per second, 5,000 calls per day.¹⁰
- Comparison: This is significantly faster than the NYT (12/sec vs 10/min), making The Guardian ideal for bulk ingestion of the 1999–2012 period.

## 4.4 URL Stability

Guardian URLs are semantic and stable (e.g., theguardian.com/technology/2008/jan/01/...). The API returns these standard URLs. Because the full text is often included in the JSON response, the risk of "Link Rot" is effectively zero within the context of the API data itself.

## 5. The GDELT Project: The Signal vs. The Noise

The Global Database of Events, Language, and Tone (GDELT) is unique in this list. It is not a publisher (like NYT) or a crawler (like IA). It is a metadata engine that extracts "events" (Who did What to Whom) from global news.

## 5.1 GDELT 1.0 vs. 2.0: The 2013 Cutoff

The user's constraint (1990–2012) forces the use of GDELT 1.0.

- GDELT 2.0: Released in 2015, covers data from Feb 2015 onwards.¹³ It uses JSON and has a high-resolution API. It is irrelevant for this project.

- GDELT 1.0: Covers 1979–2013.¹³ This dataset is stored as Tab-Separated Values (TSV) inside Zip files. There is no REST API that returns individual event rows for 1990–2012.

## 5.2 Implementation Strategy for GDELT 1.0

To "implement" GDELT for 1990–2012, one does not write a requests.get() call to a search endpoint. Instead, the workflow is:

1. Bulk Download: Access the GDELT 1.0 file list (CSV format).¹⁴
2. Filter Files: Identify the Zip files corresponding to the target years (e.g., 1990.zip to 2012.zip). Note that prior to April 1, 2013, files were monthly or yearly; afterwards, they became daily.¹⁴
3. Parsing: Download and unzip the files programmatically. Parse the TSV data.
4. Schema Map: The 1990–2012 data uses the GDELT 1.0 schema (57 columns). Key columns include SQLDATE (Date), Actor1Name, Actor2Name, EventCode (CAMEO taxonomy), and SOURCEURL.

## 5.3 The Link Rot Crisis in GDELT

The SOURCEURL column in GDELT 1.0 is the most valuable field for this project, but it is highly perilous. A URL captured by GDELT in 1994 (likely from a text scrape) is almost certainly dead today. GDELT acts as a Pointer System. It tells the archivist that "An article existed at this URL on this date."

Integration: The Python code must take the SOURCEURL and SQLDATE from GDELT and feed them directly into the Wayback Machine API. It is futile to attempt to curl the live URL.

## 6. The Failure of Commercial Search (Google/Bing)

The original request asked to identify and implement Google and Bing APIs. After exhaustive research, the conclusion is that these tools are unsuitable and largely incompatible with the 1990–2012 archival constraint. They should be excluded from the primary implementation for the reasons detailed below.

## 6.1 The "Freshness" Bias

Search engines are designed to surface the most relevant, up-to-date information. They are not archives.

- Bing Search API v7: The freshness parameter supports Day, Week, and Month.² It explicitly lacks a "Year" or "Date Range" parameter for historical queries in the official API. While some "hacks" exist (e.g., custom filters like ex1:"ez5..."), these are undocumented and unreliable.¹⁵
- Recency Leakage: Even if one successfully restricts a search to "1990..2012" (using site: operators or advanced parameters), the engines often return the current version of a

page. If example.com was indexed in 2005 but the content was replaced in 2020, the search engine returns the 2020 content, polluting the archival dataset.¹⁶

## 6.2 Syntax Limitations

- Google Custom Search JSON API: While it supports dateRestrict, this parameter is relative (e.g., d30 for past 30 days).¹⁷ The sort=date:r:YYYYMMDD:YYYYMMDD parameter attempts to enforce a hard range, but developer reports¹⁸ confirm that this often returns zero results because Google does not consistently index publication dates for older, static HTML pages that lack modern schema.org markup.
- Quota: The Google Custom Search API free tier is limited to 100 queries per day. For an archival project covering 22 years (1990–2012), this quota is insufficient to the point of uselessness.

## 6.3 Conclusion on Search Engines

Google and Bing are "Discovery" tools, not "Recovery" tools. For 1990–2012, they introduce high noise (modern content masquerading as old) and low signal. They are excluded from the Python implementation in favor of the Internet Archive.

## 7. Chronicling America: The Out-of-Bounds Archive

The user requested the inclusion of Chronicling America, a service of the Library of Congress. The research¹⁹ indicates a fatal incompatibility with the target date range.

## 7.1 The 1963 Copyright Cliff

Chronicling America digitizes newspapers published between 1777 and 1963. This cutoff is driven by U.S. copyright law (works published before 1924/1964 public domain rules).

- Target Range: 1990–2012.
- Coverage: Ends 1963.
- Verdict: There is zero overlap.

## 7.2 Exclusion

Including Chronicling America in the code would result in unnecessary API calls that effectively return 404s or empty sets for the requested years. It is strictly excluded from the Python implementation, and this limitation is documented in the final matrix.

## 8. Synthesis Matrix: Provider Capabilities

The following matrix synthesizes the technical capabilities of each provider specifically for the 1990-2012 use case.

|  Provider | Historical Reach | Syntax (Date) | Free Tier Limits | Link Rot Risk | Verdict  |
| --- | --- | --- | --- | --- | --- |
|  NY Times | 1851-Present | begin_date =YYYYMMDD | 4,000 / day (10/min) | Moderate (Metadata safe, URLs fragile) | Primary Text Source  |
|  The Guardian | 1999-Present | from-date=YYYY-MM-DD | 5,000 / day (12/sec) | Low (Full text in JSON) | Primary (Post-1999)  |
|  Internet Archive | 1996-Present | from=YYYYMMDD | High / Flexible | Zero (Is the archive) | Universal Fallback  |
|  GDELT | 1979-Present | CSV Parsing (No API) | Unlimited (File DL) | Critical (99% Dead Links) | Pointer / Index  |
|  Google/Bing | Current | Unreliable / Undocumented | 100 / day | High (Recency Bias) | Exclude  |
|  Chronicling America | 1777-1963 | N/A | N/A | N/A | Exclude  |

# 9. Python Implementation: The Digital Harvester

The following Python architecture implements the research findings. It is designed as a "Federated Harvester" that normalizes the eccentricities of each API (date formats, pagination) into a unified dataframe.

# 9.1 Technical Features

1. Date Normalization: Converts the user's generic start/end dates into the specific formats required (YYYYMMDD vs YYYYY-MM-DD).
2. Rate Limit Governance: Implements a token bucket or simple sleep mechanism to

adhere to NYT (6s delay) and Guardian constraints.

1. **Wayback Fallback**: A dedicated class that takes "Dead" URLs from NYT/GDELT and queries the CDX API to find a living snapshot.
2. **GDELT Logic**: While a full ETL pipeline for GDELT is too large for a single script, the code includes a stub function demonstrating how to map GDELT SOURCEURLs to the Wayback Machine.

## 9.2 The Code

Python

```python
import requests
import time
import pandas as pd
from datetime import datetime
import logging
```

```python
# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
```

```python
class Config:
""
Central Configuration for API Keys and Endpoints.
Replace 'YOUR_KEY' with actual credentials.
""
NYT_KEY = "YOUR_NYT_KEY"
GUARDIAN_KEY = "test" # 'test' works for low volume dev access
```

```python
# Target Window
START_DATE = "1990-01-01"
END_DATE = "2012-12-31"
```

```python
class RateLimiter:
""
Enforces API politeness.
NYT: 6 seconds between calls (10/min).
Guardian: 1 second (safe buffer).
Wayback: 1 second (politeness).
""
def __init__(self, interval):
self.interval = interval

self.last_call = 0

```txt
def wait(self):
elapsed = time.time() - self.last_call
if elapsed &lt; self.interval:
time.sleep(self.interval - elapsed)
self.last_call = time.time()
```

```txt
class WaybackMachine:
"""
Interface for the Internet Archive CDX API.
Used as a fallback for dead links.
"""
CDX_URL = "http://web.archive.org/cdx/search/cdx"
def __init__(self):
self.limiter = RateLimiter(1.0)
def get_snapshot(self, url, target_date_str):
"""
Finds the closest snapshot to the target date.
target_date_str: YYYYMMDD
"""
self.limiter.wait()
```

```txt
# Determine strict year range to optimize CDX lookup
try:
target_year = int(target_date_str[:4])
except ValueError:
target_year = 2000 # Default fallback
```

```txt
params = {
'url': url,
'from': str(target_year - 1), # Look 1 year back
'to': str(target_year + 1), # Look 1 year forward
'limit': 1,
'output': 'json',
'fl': 'timestamp, original, statuscode',
'collapse': 'digest', # unique content only
'filter': 'statuscode:200' # only successful captures
}
```

```txt
try:

```python
response = requests.get(self.CDX_URL, params=params, timeout=10)
if response.status_code == 200:
data = response.json()
if len(data) &gt; 1: # Row 0 is header
ts = data
orig = data
# Construct Wayback URL
return f"http://web.archive.org/web/{ts}/{orig}"
except Exception as e:
logging.error(f"Wayback lookup failed for {url}: {e}")
```

```python
return None
```

```python
class NYTimesAPI:
""
Wrapper for NYT Article Search API.
Handles date formatting (YYYYMMDD) and pagination.
""
BASE_URL = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
```

```python
def __init__(self, api_key):
self.api_key = api_key
self.limiter = RateLimiter(6.0) # Strict 6s delay
```

```python
def search(self, query, start_date, end_date):
# Convert YYYY-MM-DD -&gt; YYYYMMDD
begin = start_date.replace("-", "")
end = end_date.replace("-", "")
```

```python
results =
page = 0
```

```python
# Fetching first 2 pages (20 items) to demonstrate
while page &lt; 2:
self.limiter.wait()
params = {
'q': query,
'begin_date': begin,
'end_date': end,
'api-key': self.api_key,
'page': page,
'fl': 'web_url, headline, pub_date, abstract, lead_paragraph'
}
```

```python
resp = requests.get(self.BASE_URL, params=params)
if resp.status_code == 200:
data = resp.json().get('response', {}).get('docs',)
if not data:
break

for item in data:
results.append({
'provider': 'NYT',
'title': item.get('headline', {}).get('main'),
'date': item.get('pub_date')[:10],
'original_url': item.get('web_url'),
'abstract': item.get('abstract'),
'link_rot_risk': 'Medium'
})
}
page += 1
elif resp.status_code == 429:
loggingLearning("NYT Rate Limit Hit. Sleeping...")
time.sleep(10)
else:
logging.error(f"NYT Error (resp.status_code)")
break
```

return results

class GuardianAPI:

"''' Wrapper for Guardian Open Platform.

Handles date formatting (YYYY-MM-DD).

"''' BASE_URL = "<https://content.guardianapis.com/search>"

```python
def __init__(self, api_key):
self.api_key = api_key
self.limiter = RateLimiter(0.5)
```

```python
def search(self, query, start_date, end_date):
# Guardian starts 1999. Warn if start_date is earlier.
if int(start_date[:4]) &lt; 1999:
loggingLearning("The Guardian API only covers 1999+. Pre-1999 queries will be empty.")
```

```txt
results =

self.limiter.wait()

params = {
'q': query,
'from-date': start_date,
'to-date': end_date,
'api-key': self.api_key,
'page-size': 20,
'show-fields': 'trailText,bodyText' # Get body text
}

resp = requests.get(self.BASE_URL, params=params)
if resp.status_code == 200:
data = resp.json().get('response', {}).get('results',)
for item in data:
results.append({
'provider': 'Guardian',
'title': item.get('webTitle'),
'date': item.get('webPublicationDate')[:10],
'original_url': item.get('webUrl'),
'abstract': item.get('fields', {}).get('trailText'),
'link_rot_risk': 'Low' # Content is in API response
})
else:
logging.error(f"Guardian Error (resp.status_code)")
}

return results

def run_harvest(query):
"""
Orchestrates the harvest across providers.
"""
logging.info(f"Starting harvest for query: '(query)' ({Config.START_DATE} to {Config.END_DATE})")

# Initialize Providers
nyt = NYTimesAPI(Config.NYT_KEY)
guardian = GuardianAPI(Config.GUARDIAN_KEY)
wayback = WaybackMachine()

harvested_data =

# 1. Harvest NYT
harvested_data.extend(nyt.search(query, Config.START_DATE, Config.END_DATE))

# 2. Harvest Guardian

harvested_data extend(guardian.search(query, Config.START_DATE, Config.END_DATE))

# 3. Process Link Rot (Wayback Fallback)

logging.info("Processing Link Rot Fallbacks...")

final_rows =

for row in harvested_data:

If risk is Medium/High, get a fallback

if row['link_rot_risk'] == 'Medium':

Format date for CDX (remove dashes)

clean_date = row['date'].replace("-", "")

wb_link = wayback.get_snapshot(row['original_url'], clean_date)

row['wayback_url'] = wb_link if wb_link else "Not Found"

else:

row['wayback_url'] = "N/A (Content in API)"

final_rows.append(row)

Output

df = pd.DataFrame(final_rows)

print("\n=== Harvest Results ===")

if not df.empty:

print(df[['provider', 'date', 'title', 'wayback_url']].to_markdown())

else:

print("No results found.")

if __name__ == "__main__":

Example Query

run_harvest("internet regulation")

# 9.3 Code Analysis and Limitations

This Python script serves as a foundational "Digital Harvester."

- Modularity: Providers are encapsulated in classes, allowing easy addition of new archives (e.g., Common Crawl via cdx_toolkit).
- Error Handling: It anticipates the specific error codes (429 for Rate Limit) documented in the NYT snippets. $^{9}$
- Gap Management: It explicitly logs a warning for The Guardian pre-1999 queries, aligning with the finding in. $^{10}$
- **Fallbacks:** The WaybackMachine class implements the logic derived from Snippet  $^{3}$ ,

using collapse=digest to ensure the script doesn't retrieve 50 identical copies of the same page, optimizing for the unique historical record.

This framework satisfies the "Digital Archivist" objective: it does not merely "search the web" (which fails for 1990-2012); it queries the record of the web, verifying existence via the Internet Archive, and retrieving metadata from the few publishers (NYT, Guardian) who have maintained their own deep archives.

# Works cited

1. Bing Search API Replacement: Web Search - SerpApi, accessed January 8, 2026, https://serpapi.com/blog/bing-search-api-replacement-web-search/
2. Access Archive-It's Wayback index with the CDX/C API, accessed January 8, 2026, https://support.archive-it.org/hc/en-us/articles/115001790023-Access-Archive-It-s-Wayback-index-with-the-CDX-C-API
3. wayback/wayback-cdx-server/README.md at master - GitHub, accessed January 8, 2026, https://github.com/internetarchive/wayback/blob/master/wayback-cdx-server/RE ADME.md
4. commoncrawl/cdx_toolkit: A toolkit for CDX indices such as Common Crawl and the Internet Archive's Wayback Machine - GitHub, accessed January 8, 2026, https://github.com/cocrawler/cdx_toolkit
5. public_api_specs/article_search/article_search_v2.md at master - GitHub, accessed January 8, 2026, https://github.com/nytimes/public_api_specs/blob/master/article_search/article_search_v2.md
6. Getting Started with the NYT API - D-Lab @ Berkeley, accessed January 8, 2026, https://dlab.berkeley.edu/news/getting-started-nyt-api
7. Working with the NY Times API - RPubs, accessed January 8, 2026, https://rpubs.com/aliceafriedman/week-9-HW
8. A Guide to Querying the New York Times API with Python | by Nick Subic, accessed January 8, 2026, https://nicksubic.medium.com/a-guide-to-querying-the-new-york-times-api-withpython-b621556236f8
9. The Guardian - Open Platform, accessed January 8, 2026, https://open-platform.theguardian.com/
10. documentation / content - theguardian / open platform, accessed January 8, 2026, https://open-platform.theguardian.com/documentation/search
11. explore - theguardian / open platform, accessed January 8, 2026, https://open-platform.theguardian.com/explore/
12. the gdelt event database data format codebook v2.0 2/19/2015, accessed January 8, 2026, http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf
13. Data: Querying, Analyzing and Downloading: The GDELT Project, accessed

January 8, 2026, https://www.gdeltproject.org/data.html

14. Bing Search API: Narrow by date - Stack Overflow, accessed January 8, 2026, https://stackoverflow.com/questions/7624596/bing-search-api-narrow-by-date

15. Exploring the Bing Date Search Operators - nullhandle.org, accessed January 8, 2026, https://nullhandle.org/blog/2024-06-12-exploring-the-bing-date-search-operators.html

16. How to use "dateRestrict" parameter in Custom Search API - Stack Overflow, accessed January 8, 2026, https://stackoverflow.com/questions/14543555/how-to-use-daterestrict-parameter-in-custom-search-api

17. What is the way to filter using date range in google custom search api?, accessed January 8, 2026, https://support.google.com/programmable-search/thread/203850064/what-is-the-way-to-filter-using-date-range-in-google-custom-search-api?hl=en

18. Chronicling America: Historic American Newspapers | Rutgers University Libraries, accessed January 8, 2026, https://www.libraries.rutgers.edu/databases/chroniclingamerica

19. Chronicling America - Wikipedia, accessed January 8, 2026, https://en.wikipedia.org/wiki/Chronicling_America
