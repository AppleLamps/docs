# Quick Reference Guide - Historical APIs Toolkit

**Date:** January 2026 | **Coverage:** 1990â€“2012 research

---

## ğŸš€ One-Command Setup

```bash
pip install requests google-cloud-bigquery
```

---

## 5 APIs at a Glance

### 1. Internet Archive CDX âœ… BEST FOR: Web snapshots

```python
cdx = InternetArchiveCDX()
results = cdx.query("example.com", 1990, 2012)
```

**Why:** Only source for complete web history (1996+)  
**Cost:** Free, unlimited | **Auth:** None  
**Link Rot:** âœ… Zero risk | **30-Day Trap:** âœ… Safe

---

### 2. New York Times Article Search âœ… BEST FOR: Quality US news

```python
nyt = NYTArticleSearch(api_key="YOUR_KEY")
articles = nyt.iterate_all_results("climate change", 1990, 2012)
```

**Why:** Highest-quality structured content (1851+)  
**Cost:** Free (4K/day) | **Auth:** API key (5 min setup)  
**Link Rot:** âœ… Low risk | **30-Day Trap:** âœ… Safe

Get key: https://developer.nytimes.com/

---

### 3. Chronicling America âœ… BEST FOR: Historic newspapers (1690â€“1963)

```python
ca = ChroniclingAmerica()
results = ca.query("Civil War", 1860, 1865)
```

**Why:** ONLY source for 1690â€“1963 full-text  
**Cost:** Free, unlimited | **Auth:** None  
**Link Rot:** âœ… Zero risk | **30-Day Trap:** âœ… Safe

---

### 4. GDELT BigQuery âœ… BEST FOR: Global event data

```python
gdelt = GDELTBigQuery("path/to/keyfile.json")
events = gdelt.query_events(1990, 2012, event_codes=["0211"])
```

**Why:** 35+ years of structured newsflow (1979+)  
**Cost:** Free (1TB/month) | **Auth:** Google Cloud  
**Link Rot:** âš ï¸ Source links may rot | **30-Day Trap:** âš ï¸ Minimal

**Setup:** 30 min via https://console.cloud.google.com

---

### 5. The Guardian API âœ… BEST FOR: International perspective (1999+)

```python
guardian = GuardianAPI(api_key="YOUR_KEY")
articles = guardian.iterate_all_results("world bank", 1999, 2012)
```

**Why:** Non-US viewpoint, generous free tier, fast  
**Cost:** Free (1M/month!) | **Auth:** API key (5 min setup)  
**Link Rot:** âœ… Low risk | **30-Day Trap:** âœ… Safe  
**Limitation:** No coverage before 1999 âŒ

Get key: https://open-platform.theguardian.com/

---

## ğŸ¯ Decision Tree: Which API to Use?

```
Is your query about...?

1. SPECIFIC TIME PERIOD?
   â”œâ”€ Before 1690? â†’ Not in any API
   â”œâ”€ 1690â€“1963?  â†’ Chronicling America (only source!)
   â”œâ”€ 1963â€“1999?  â†’ Internet Archive CDX + GDELT
   â”œâ”€ 1999â€“2012?  â†’ CDX + NYT + GDELT + Guardian
   â””â”€ 2012+?      â†’ All APIs work

2. LOOKING FOR FULL-TEXT?
   â”œâ”€ Yes â†’ Chronicling America (newspapers) or Archive snapshots
   â””â”€ No  â†’ NYT snippets, GDELT metadata, Guardian abstracts

3. NEED STRUCTURED ANALYSIS?
   â”œâ”€ Yes â†’ GDELT BigQuery (SQL queries)
   â””â”€ No  â†’ CDX, NYT, Guardian (article-based)

4. BUDGET?
   â”œâ”€ $0, no setup â†’ CDX, Chronicling America
   â”œâ”€ $0, 5-min setup â†’ NYT, Guardian (API keys)
   â””â”€ $0, 30-min setup â†’ GDELT BigQuery (Google Cloud)
```

---

## ğŸ“Š Coverage Map

```
Time Period | CDX | NYT | CA  | GDELT | Guardian
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1690â€“1836   |  âœ—  |  âœ—  | âœ“âœ“âœ“ |   âœ—   |    âœ—
1836â€“1922   |  âœ—  |  âœ—  | âœ“âœ“âœ“ |   âœ—   |    âœ—
1922â€“1963   |  âœ—  |  âœ—  | âœ“âœ“âœ“ |   âœ—   |    âœ—
1963â€“1979   |  â—  |  â—  |  âœ—  |   âœ—   |    âœ—
1979â€“1996   |  â—  |  âœ“  |  âœ—  |  âœ“âœ“âœ“  |    âœ—
1996â€“1999   | âœ“âœ“  |  âœ“  |  âœ—  |  âœ“âœ“âœ“  |    âœ—
1999â€“2012   | âœ“âœ“âœ“ |  âœ“  |  âœ—  |  âœ“âœ“âœ“  |   âœ“âœ“
2012â€“now    | âœ“âœ“âœ“ | âœ“âœ“âœ“ |  âœ—  |  âœ“âœ“âœ“  |  âœ“âœ“âœ“

Legend: âœ— = No | â— = Sparse | âœ“ = Good | âœ“âœ“ = Excellent | âœ“âœ“âœ“ = Complete
```

---

## âš¡ Quick Commands Reference

### Search by keyword + date range

```python
# Single API
results = nyt.query("9/11", 2001, 2001)

# Multiple APIs
orchestrator = HistoricalNewsOrchestrator(nyt_key="...", guardian_key="...")
results = orchestrator.comprehensive_search("climate", 1990, 2012)
```

### Check web archive coverage

```python
captures = cdx.query("example.com", 1990, 2012)
print(f"Found {len(captures['captures'])} snapshots")
```

### Get permanent archive link

```python
url = cdx.build_wayback_url("example.com", "20010911")
# Output: https://web.archive.org/web/20010911/http://example.com
```

### Fallback to archive when link dies

```python
fallback = ArchiveOrgFallback()
archive_url = fallback.get_closest_snapshot("deadlink.com", 2005)
```

---

## ğŸš¨ "30-Day Trap" Prevention

### What It Is
Google/Bing re-index the web. Old pages that no one links to disappear from search results.

### How This Toolkit Avoids It
| API | Strategy |
|-----|----------|
| **CDX** | Archives static (never re-indexed) |
| **NYT** | Indexes own archive (not live web) |
| **CA** | Library of Congress (permanent) |
| **GDELT** | Events timestamped at publication |
| **Guardian** | Their archive (not live web) |

âœ… **All recommended APIs are safe from the 30-Day Trap**

---

## ğŸ“ˆ Response Times

| API | Time | Notes |
|-----|------|-------|
| **CDX** | <1 sec | Instant |
| **NYT** | 1â€“5 sec | REST API |
| **CA** | <1 sec | Instant |
| **GDELT BigQuery** | 5â€“30 sec | SQL query |
| **Guardian** | <1 sec | Instant |

---

## ğŸ’¾ Data Export

### To CSV
```python
import csv
with open("results.csv", "w") as f:
    writer = csv.DictWriter(f, fieldnames=['date', 'title', 'url'])
    writer.writeheader()
    writer.writerows(articles)
```

### To JSON
```python
import json
with open("results.json", "w") as f:
    json.dump(articles, f, indent=2)
```

### To Pandas DataFrame
```python
import pandas as pd
df = pd.DataFrame(articles)
df.to_csv("results.csv", index=False)
```

---

## ğŸ”‘ API Keys Needed

| API | Required? | Get Here |
|-----|-----------|----------|
| CDX | âŒ No | None |
| NYT | âœ… Yes | https://developer.nytimes.com/ |
| CA | âŒ No | None |
| GDELT v1 | âŒ No | None |
| GDELT BigQuery | âœ… Yes | https://console.cloud.google.com |
| Guardian | âœ… Yes | https://open-platform.theguardian.com/ |

**Total setup time:** 30 minutes

---

## âœ… Sanity Check Your Query

- [ ] Date range: 1690â€“2012 âœ“
- [ ] API supports your time period âœ“
- [ ] API key valid (if required) âœ“
- [ ] Query text is reasonable (1â€“5 keywords) âœ“
- [ ] Rate limits respected (delays added) âœ“
- [ ] Response format understood (JSON/CSV) âœ“
- [ ] Results exported/saved âœ“

---

## ğŸ†˜ Troubleshooting in 30 Seconds

| Problem | Solution |
|---------|----------|
| "Connection timeout" | Increase delay: `rate_limit_delay=2.0` |
| "API key invalid" | Verify key in web console, ensure API enabled |
| "No results" | Check date range supported, try different query |
| "Too many requests" | Add delays: `rate_limit_delay=2.0` |
| "BigQuery auth error" | Verify service account has BigQuery User role |
| "Link returns 404" | Use: `fallback.get_closest_snapshot(url, year)` |

---

## ğŸ“š Next Steps

1. **Install:** `pip install requests google-cloud-bigquery`
2. **Pick API:** Use decision tree above
3. **Get keys:** Register for free keys (5 min each)
4. **Run example:** Copy code matching your API
5. **Iterate:** Adjust date range, query terms
6. **Export:** Save results to CSV/JSON

---

**Status:** âœ… Production-ready | **Python:** 3.7+ | **License:** MIT

